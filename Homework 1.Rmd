---
title: "Homework 1"
author: "Abhishek Kumar"
date: "02/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, results = "hide", message = FALSE}
rm(list = ls())
graphics.off()

# install.packages("arules")
# install.packages("gtools")

library(ISLR2)
library(arules)
library(gtools)
library(rpart)
library(rpart.plot)
```

1) Consider the “College” data in the ISLR2 package: > library(ISLR2)
> data(College)
> head(College)

```{r}
data(College)
my_College <- as.data.frame(College)
head(my_College)

colnames(my_College)
```

a) Present some visualizations of this data such as pair plots and histograms? Do you think any scaling or transformation is required?
<br>
- Many variables have a very high range and some show pair-wise nonlinear relationships. I'm using log transform to rescale those variables.

```{r}

# piarplots
plot(my_College[, 1:6])
plot(my_College[, 7:12])
plot(my_College[, 13:18])
```

```{r}
# histograms
par(mfrow=c(2,2))
hist(my_College$Apps, breaks=20, col = "red", main="Apps")
hist(my_College$Accept, breaks=20, col = "red", main="Accept")
hist(my_College$Enroll, breaks=20, col = "red", main="Enroll")
hist(my_College$Top10perc, breaks=20, col = "red", main="Top10perc")

par(mfrow=c(2,2))
hist(my_College$Top25perc, breaks=20, col="orange", main="Top25perc")
hist(my_College$F.Undergrad, breaks=20, col="orange", main="F.Undergrad")
hist(my_College$P.Undergrad, breaks=20, col="orange", main="P.Undergrad")
hist(my_College$Outstate, breaks=20, col="orange", main="Outstate")

par(mfrow=c(2,2))
hist(my_College$Room.Board, breaks=20, col="green", main="Room.Board")
hist(my_College$Books, breaks=20, col="green", main="Books")
hist(my_College$Personal, breaks=20, col="green", main="Personal")
hist(my_College$PhD, breaks=20, col="green", main="PhD")

par(mfrow=c(2,2))
hist(my_College$Terminal, breaks=20, col="blue", main="Terminal")
hist(my_College$S.F.Ratio, breaks=20, col="blue", main="S.F.Ratio")
hist(my_College$perc.alumni, breaks=20, col="blue", main="perc.alumni")
hist(my_College$Expend, breaks=20, col="yellow", main="Expend")

par(mfrow=c(2,2))
hist(my_College$Grad.Rate, breaks=20, col="yellow", main="Grad.Rate")

```

b) Scale the data appropriately (e.g., log transform) and present the visualizations in part A. Have any new relationships been revealied.
<br>
- We can see now that the distributions are more or less normal now. Also, the variables that were showing slightly non-linear relationships are now showing linear relationships (e.g., Top25perc and Top10perc).

```{r}
# summary(my_College)

for (i in 2:dim(my_College)[2]){
  if (min(my_College[, i])/max(my_College[, i]) < 0.05){
    my_College[, i] <- log(my_College[, i] + 1)
  }
}

summary(my_College)

plot(my_College[, 1:6])
plot(my_College[, 7:12])
plot(my_College[, 13:18])
```

```{r}
# histograms
par(mfrow=c(2,2))
hist(my_College$Apps, breaks=20, col = "red", main="Apps")
hist(my_College$Accept, breaks=20, col = "red", main="Accept")
hist(my_College$Enroll, breaks=20, col = "red", main="Enroll")
hist(my_College$Top10perc, breaks=20, col = "red", main="Top10perc")

par(mfrow=c(2,2))
hist(my_College$Top25perc, breaks=20, col="orange", main="Top25perc")
hist(my_College$F.Undergrad, breaks=20, col="orange", main="F.Undergrad")
hist(my_College$P.Undergrad, breaks=20, col="orange", main="P.Undergrad")
hist(my_College$Outstate, breaks=20, col="orange", main="Outstate")

par(mfrow=c(2,2))
hist(my_College$Room.Board, breaks=20, col="green", main="Room.Board")
hist(my_College$Books, breaks=20, col="green", main="Books")
hist(my_College$Personal, breaks=20, col="green", main="Personal")
hist(my_College$PhD, breaks=20, col="green", main="PhD")

par(mfrow=c(2,2))
hist(my_College$Terminal, breaks=20, col="blue", main="Terminal")
hist(my_College$S.F.Ratio, breaks=20, col="blue", main="S.F.Ratio")
hist(my_College$perc.alumni, breaks=20, col="blue", main="perc.alumni")
hist(my_College$Expend, breaks=20, col="blue", main="Expend")

par(mfrow=c(2,2))
hist(my_College$Grad.Rate, breaks=20, col="yellow", main="Grad.Rate")
```

c) Subset the data into two data frames: “private” and “public”. Save them as an *.RData file. Be sure these are the only two objects saved in that file. Submit it with you assignement.

```{r}
private <- my_College[my_College['Private'] == 'Yes', ]
public <- my_College[my_College['Private'] == 'No', ]
my_list <- list(private, public)
save(my_list, file = "hw1q1c.RData")
```

** For the remaining parts – use the “private and public” datasets. **
d) Within each new data frame, sort the observations in decreasing order by
number of applications recieved.

```{r}
private <- private[order(-private$Apps), ]
public <- public[order(-public$Apps), ]
```

e) Eliminate Universities that have less than the median number of HS students
admitted from the top 25% of the class (“Top25perc”).

```{r}
my_private <- private[private$Top25perc >= median(private$Top25perc), ]
my_public <- public[public$Top25perc >= median(public$Top25perc), ]
```

f) Create a new variable that categorizes graduation rate into “High”, “Medium”
and “Low”, use a histogram or quantiles to determine how to create this
variable. Append this variable to your “private” and “public” datasets.

```{r}
Grad.Rate.Group <- quantile(my_College$Grad.Rate, probs=c(0.33, 0.66))
my_private$Grad.Rate.Group <- ifelse(my_private$Grad.Rate <= Grad.Rate.Group[1], "Low", ifelse(my_private$Grad.Rate > Grad.Rate.Group[2], "High", "Medium"))
my_public$Grad.Rate.Group <- ifelse(my_public$Grad.Rate <= Grad.Rate.Group[1], "Low", ifelse(my_public$Grad.Rate > Grad.Rate.Group[2], "High", "Medium"))
```

g) Create a “list structure” that contains your two datasets and save this to an
*.RData file. Make sure that your file contains only the list structure.

```{r}
my_list2 <- list(my_private, my_public)
save(my_list2, file = "hw1q1g.RData")
```

##
##
##

2) You are going to derive generalized association rules to the marketing data from your book ESL. This data is in the available on UB learns. Specifically, generate a reference sample of the same size of the training set. This can be done in a couple of ways, e.g., (i) sample uniformly for each variable, or (ii) by randomly permuting the values within each variable independently. Build a classification tree to the training sample (class 1) and the reference sample (class 0) and describe the terminal nodes having highest estimated class 1 probability. Compare the results to the results near Table 14.1 (ESL), which were derived using PRIM.

```{r}
set.seed(888)

load('~/Desktop/UB Courses/Sem 2/SDM II/Homeworks/marketing.RData')
# summary(marketing)
# sum(rowSums(is.na(marketing)) >= 1)

my_marketing <- na.omit(marketing)
my_marketing <- lapply(my_marketing, function(x){ factor(x, ordered=TRUE) })
my_marketing <- as.data.frame(my_marketing)
# summary(my_marketing)

ref_sample <- lapply(my_marketing, function(x){ sample(x, replace=TRUE) })
ref_sample <- as.data.frame(ref_sample)

# summary(ref_sample)
```

```{r}
my_marketing$class <- 1
ref_sample$class <- 0

my_data <- rbind(my_marketing, ref_sample)

model.control <- rpart.control(minsplit = 500, xval = 20, cp = 0)
cart.fit <- rpart(class ~ ., data = my_data, method = "class", control = model.control)

min_cp = which.min(cart.fit$cptable[,4])
pruned.cart.fit <- prune(cart.fit, cp = cart.fit$cptable[min_cp,1])

plot(cart.fit, branch = 0.5, uniform = TRUE, compress = TRUE)
text(cart.fit, use.n = TRUE, all = TRUE, cex = 0.4)

rpart.plot(pruned.cart.fit, type = 5, extra = 104, cex = 0.4)
```
```{r}
# Make predictions
y_pred = predict(pruned.cart.fit, my_data)
# y_pred

# Comparing to the rules from ESL near Table 14.1
my_rules <- my_data[which(y_pred[, 2]>0.1), ]

rule1 <- my_rules[my_rules$Household == 1 & my_rules$Language == 1, ]
length(rule1[, 1])

rule2 <- my_rules[my_rules$Language == 1 & my_rules$Status == 1 & my_rules$Occupation == 1, ]
length(rule2[, 1])

rule3 <- my_rules[my_rules$Marital == 2 & my_rules$Income < 7 & my_rules$Language == 1 & my_rules$Household == 2 & my_rules$Edu < 4, ]
length(rule3[, 1])

```

##
##
##

3) Consider the Boston Housing Data in the ISLR2 package.

```{r}
data(Boston)
my_Boston <- as.data.frame(Boston)
summary(my_Boston)
str(my_Boston)
```

a) Visualize the data using histograms of the different variables in the data set. Transform the data into a binary incidence matrix, and justify the choices you make in grouping categories.

```{r}
par(mfrow=c(2,2))
hist(my_Boston$crim, breaks=20, col="red", main="crim")
hist(my_Boston$zn, breaks=20, col="orange", main="zn")
hist(my_Boston$indus, breaks=20, col="green", main="indus")
hist(my_Boston$chas, breaks=20, col="blue", main="chas")

par(mfrow=c(2,2))
hist(my_Boston$nox, breaks=20, col="red", main="nox")
hist(my_Boston$rm, breaks=20, col="orange", main="rm")
hist(my_Boston$age, breaks=20, col="green", main="age")
hist(my_Boston$dis, breaks=20, col="blue", main="dis")

par(mfrow=c(2,2))
hist(my_Boston$rad, breaks=20, col="red", main="rad")
hist(my_Boston$tax, breaks=20, col="orange", main="tax")
hist(my_Boston$ptratio, breaks=20, col="green", main="ptratio")
hist(my_Boston$lstat, breaks=20, col="blue", main="lstat")

par(mfrow=c(2,2))
hist(my_Boston$medv, breaks=20, col="red", main="medv")
```

```{r}
# chas and rad are ordinal. I'm going to convert them to factors
my_Boston$chas <- factor(my_Boston$chas)
my_Boston$rad <- factor(my_Boston$rad)

# discretising the other columns using discretize function
for (col in colnames(my_Boston)){
  if (col != 'chas' & col!= 'rad'){ # already converted to factor
  # I'm dividing the other variables in 4 equal parts since there aren't any bimodal or multimodal distriutions
    my_Boston[[col]] <- discretize(my_Boston[[col]], method = 'interval', breaks = 4, labels = c("low", "low-med", "med-high", "high"), ordered_result = TRUE)
  } 
}

my_trans <- as(my_Boston, "transactions")

summary(my_trans)
```

b) Visualize the data using the itemFrequencyPlot in the “arules” package. Apply the apriori algorithm (Do not forget to specify parameters in your write up).

```{r}
itemFrequencyPlot(my_trans, support=0.01, cex.names=0.5)
rules <- apriori(my_trans, parameter = list(support = 0.1, confidence = 0.1))
summary(rules)
```

c) A student is interested is a low crime area, but wants to be as close to the city as possible (as measured by “dis”). What can you advise on this matter through the mining of association rules?
<br>
- From the rules mentioned below we can notice that closeness to the city itself results in low crime area. Some other factors are age and indus.

```{r}
LowCrimeDis <- subset(rules, subset = (rhs %in% c("crim=low") & lhs %in% c("dis=low")))
inspect(head(LowCrimeDis))
inspect(head(sort(LowCrimeDis, by = "confidence"), n = 3))
inspect(head(sort(LowCrimeDis, by = "lift"), n = 3))
```

d) A family is moving to the area, and has made schooling a priority. They want schools with low pupil-teacher ratios. What can you advise on this matter through
the mining of association rules?
<br>
- Areas with low indus, low crim and 0 chas have a low-mid ptratio
```{r}
LowPTR <- subset(rules, subset = (rhs %in% c("ptratio=low") | rhs %in% c("ptratio=low-med")))
inspect(LowPTR)
inspect(head(sort(LowPTR, by = "confidence"), n = 3))
inspect(head(sort(LowPTR, by = "lift"), n = 3))
```

Extra Credit: Use a regression model to solve part d. Are you results comparable? Which provides an easier interpretation? When would regression be preferred, and when would association models be preferred?
<br>
- The regression model shows that the variables 'zn', 'nox', 'rad' and 'medv' are most important factors in determining ptratio. The adjusted R squared value is pretty low however.
<br>
- Regression model captures the effect of continuous variables well. Since we have converted the continuous variables in discrete bins, the association models haven't been able to capture some relationships well. This relationship could probably have been improved by increasing the number of bins
<br>
- Association models should be preferred when there are many unordered categorical variables
```{r}
model <- lm(ptratio ~ ., data = Boston)
summary(model)
```






